{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Citation Evaluation\n",
    "\n",
    "This notebook evaluates how well citations support scientific claims by:\n",
    "1. Loading citation data and references from the JSON file\n",
    "2. Using an LLM to determine if the referenced content supports each claim\n",
    "3. Evaluating multiple citations holistically when present\n",
    "4. Saving the evaluation results to evaluation.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from typing import List, Dict, Optional, Any\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from pydantic import BaseModel, Field\n",
    "from openai import OpenAI\n",
    "\n",
    "# Set your OpenAI API key\n",
    "openai_api_key = os.environ.get(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Evaluation Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reference(BaseModel):\n",
    "    \"\"\"Reference information\"\"\"\n",
    "    reference_id: int = Field(..., description=\"The ID of the reference\")\n",
    "    reference_text: str = Field(..., description=\"The full text of the reference\")\n",
    "\n",
    "class ClaimEvaluation(BaseModel):\n",
    "    \"\"\"Evaluation of whether citations support a claim\"\"\"\n",
    "    claim_text: str = Field(..., description=\"The text of the claim\")\n",
    "    citation_keys: List[int] = Field(..., description=\"Citation keys supporting this claim\")\n",
    "    citation_text: str = Field(..., description=\"Original citation text (e.g., '$^{1,2}$')\")\n",
    "    references: List[Reference] = Field(..., description=\"Full reference information for citations\")\n",
    "    is_adequately_supported: bool = Field(..., description=\"Whether the claim is adequately supported by its citations\")\n",
    "    explanation: str = Field(..., description=\"Explanation of why the citations do or don't support the claim\")\n",
    "    suggestions: Optional[List[str]] = Field(None, description=\"Suggestions for improving the citation support\")\n",
    "\n",
    "class EvaluationResults(BaseModel):\n",
    "    \"\"\"Overall evaluation results for a document\"\"\"\n",
    "    document_id: str = Field(..., description=\"ID of the document being evaluated\")\n",
    "    evaluations: List[ClaimEvaluation] = Field(..., description=\"List of claim evaluations\")\n",
    "    processed_date: str = Field(..., description=\"Date and time of evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Citation Data and References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load citations and references data from JSON file\n",
    "citation_json_path = \"citation_analysis/citations.json\"\n",
    "\n",
    "with open(citation_json_path, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "document_id = data['document_id']\n",
    "citations = data['citations']\n",
    "references = data.get('references', [])\n",
    "\n",
    "# Create a reference lookup dictionary\n",
    "reference_dict = {}\n",
    "for ref in references:\n",
    "    reference_dict[ref['reference_id']] = ref['reference_text']\n",
    "\n",
    "print(f\"Loaded {len(citations)} citations and {len(references)} references from document: {document_id}\")\n",
    "\n",
    "# Count single vs multiple citation claims\n",
    "single_citations = sum(1 for c in citations if len(c['citation_keys']) == 1)\n",
    "multi_citations = sum(1 for c in citations if len(c['citation_keys']) > 1)\n",
    "print(f\"Single citation claims: {single_citations}\")\n",
    "print(f\"Multiple citation claims: {multi_citations}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_claim(claim_data, reference_dict, references_list, client, model=\"gpt-4.1\"):\n",
    "    \"\"\"\n",
    "    Evaluate if citations support a claim.\n",
    "    \n",
    "    Args:\n",
    "        claim_data: Dictionary with claim information and citations\n",
    "        reference_dict: Dictionary mapping reference IDs to reference text\n",
    "        references_list: List of all reference objects\n",
    "        client: OpenAI client\n",
    "        model: Model to use for evaluation\n",
    "        \n",
    "    Returns:\n",
    "        ClaimEvaluation object\n",
    "    \"\"\"\n",
    "    # Extract claim information\n",
    "    claim_text = claim_data['claim']\n",
    "    citation_text = claim_data['citation_text']\n",
    "    citation_keys = claim_data['citation_keys']\n",
    "    \n",
    "    # Get the referenced content for each citation key\n",
    "    reference_content = \"\"\n",
    "    for i, key in enumerate(citation_keys):\n",
    "        ref_text = reference_dict.get(key, \"Reference content not found\")\n",
    "        reference_content += f\"Reference {i+1} (#{key}): {ref_text}\\n\\n\"\n",
    "    \n",
    "    # Get the full reference objects for these citation keys\n",
    "    claim_references = []\n",
    "    for key in citation_keys:\n",
    "        ref_obj = next((r for r in references_list if r['reference_id'] == key), None)\n",
    "        if ref_obj:\n",
    "            claim_references.append(Reference(\n",
    "                reference_id=ref_obj['reference_id'],\n",
    "                reference_text=ref_obj['reference_text']\n",
    "            ))\n",
    "    \n",
    "    # Response model for the LLM\n",
    "    class EvaluationResponse(BaseModel):\n",
    "        is_adequately_supported: bool = Field(..., description=\"Whether the claim is adequately supported by its citations\")\n",
    "        explanation: str = Field(..., description=\"Explanation of why the citations do or don't support the claim\")\n",
    "        suggestions: Optional[List[str]] = Field(None, description=\"Suggestions for improving the citation support\")\n",
    "    \n",
    "    # System prompt for claim evaluation\n",
    "    system_prompt = \"\"\"\n",
    "    You are a citation evaluation expert with extensive knowledge of scientific literature and academic standards.\n",
    "    Your task is to assess whether a scientific claim is adequately supported by its citations.\n",
    "    \n",
    "    IMPORTANT: When a claim has multiple citations, evaluate them HOLISTICALLY as a group,\n",
    "    not just individually. Consider how they work together to support the overall claim.\n",
    "    \"\"\"\n",
    "    \n",
    "    # User prompt for evaluation\n",
    "    user_prompt = f\"\"\"\n",
    "    Evaluate whether this scientific claim is adequately supported by its citations.\n",
    "    \n",
    "    CLAIM:\n",
    "    \"{claim_text}\"\n",
    "    \n",
    "    This claim is supported by {len(citation_keys)} citation(s) appearing as: {citation_text}\n",
    "    \n",
    "    CITED REFERENCES:\n",
    "    {reference_content}\n",
    "    \n",
    "    Based on the content of these references, provide an evaluation with:\n",
    "    1. Is this claim adequately supported by these references? (Yes/No)\n",
    "    2. Explain your reasoning in detail\n",
    "    3. If support is inadequate, provide 1-2 suggestions for improvement\n",
    "    \n",
    "    If there are multiple references, remember to evaluate them HOLISTICALLY, considering\n",
    "    how they work together rather than just evaluating them individually.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Use structured output to get evaluation\n",
    "        completion = client.beta.chat.completions.parse(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            response_format=EvaluationResponse\n",
    "        )\n",
    "        \n",
    "        # Get LLM response\n",
    "        response = completion.choices[0].message.parsed\n",
    "        \n",
    "        # Create the full evaluation object\n",
    "        evaluation = ClaimEvaluation(\n",
    "            claim_text=claim_text,\n",
    "            citation_keys=citation_keys,\n",
    "            citation_text=citation_text,\n",
    "            references=claim_references,\n",
    "            is_adequately_supported=response.is_adequately_supported,\n",
    "            explanation=response.explanation,\n",
    "            suggestions=response.suggestions\n",
    "        )\n",
    "        \n",
    "        return evaluation\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating claim: {e}\")\n",
    "        \n",
    "        # Return a default evaluation in case of error\n",
    "        return ClaimEvaluation(\n",
    "            claim_text=claim_text,\n",
    "            citation_keys=citation_keys,\n",
    "            citation_text=citation_text,\n",
    "            references=claim_references,\n",
    "            is_adequately_supported=True,\n",
    "            explanation=f\"Error evaluating claim: {str(e)}\"\n",
    "        )\n",
    "\n",
    "def evaluate_all_claims(citations, reference_dict, references_list, api_key, output_path=\"citation_analysis/evaluation.json\"):\n",
    "    \"\"\"\n",
    "    Evaluate all claims and save results to JSON.\n",
    "    \n",
    "    Args:\n",
    "        citations: List of citation dictionaries\n",
    "        reference_dict: Dictionary mapping reference IDs to reference text\n",
    "        references_list: List of all reference objects\n",
    "        api_key: OpenAI API key\n",
    "        output_path: Path to save evaluation results\n",
    "        \n",
    "    Returns:\n",
    "        EvaluationResults object with all evaluations\n",
    "    \"\"\"\n",
    "    # Create OpenAI client\n",
    "    client = OpenAI(api_key=api_key)\n",
    "    \n",
    "    # Evaluate each claim\n",
    "    evaluations = []\n",
    "    total_claims = len(citations)\n",
    "    \n",
    "    for i, claim in enumerate(citations):\n",
    "        print(f\"Evaluating claim {i+1}/{total_claims}: {claim['claim'][:80]}...\" if len(claim['claim']) > 80 else f\"Evaluating claim {i+1}/{total_claims}: {claim['claim']}\")\n",
    "        \n",
    "        # Evaluate the claim\n",
    "        evaluation = evaluate_claim(claim, reference_dict, references_list, client)\n",
    "        evaluations.append(evaluation)\n",
    "        \n",
    "        # Show abbreviated result\n",
    "        result = \"Adequately supported\" if evaluation.is_adequately_supported else \"Inadequately supported\"\n",
    "        print(f\"Result: {result}\\n\")\n",
    "        \n",
    "        # Save intermediate results every 10 claims\n",
    "        if (i+1) % 10 == 0:\n",
    "            intermediate_results = EvaluationResults(\n",
    "                document_id=document_id,\n",
    "                evaluations=evaluations,\n",
    "                processed_date=datetime.now().isoformat()\n",
    "            )\n",
    "            \n",
    "            # Save to temporary file\n",
    "            temp_path = output_path.replace(\".json\", \"_partial.json\")\n",
    "            with open(temp_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(intermediate_results.model_dump_json(indent=2))\n",
    "            print(f\"Saved intermediate results ({i+1}/{total_claims} claims) to {temp_path}\")\n",
    "    \n",
    "    # Create final results object\n",
    "    results = EvaluationResults(\n",
    "        document_id=document_id,\n",
    "        evaluations=evaluations,\n",
    "        processed_date=datetime.now().isoformat()\n",
    "    )\n",
    "    \n",
    "    # Save results to JSON file\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(results.model_dump_json(indent=2))\n",
    "    \n",
    "    print(f\"\\nEvaluation complete. All results saved to {output_path}\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Evaluation on All Claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if OpenAI API key is available\n",
    "if not openai_api_key:\n",
    "    print(\"Warning: No OpenAI API key provided. Set the OPENAI_API_KEY environment variable.\")\n",
    "else:\n",
    "    # Create output directory if it doesn't exist\n",
    "    Path(\"citation_analysis\").mkdir(exist_ok=True)\n",
    "    \n",
    "    # Evaluate all claims and save results\n",
    "    results = evaluate_all_claims(citations, reference_dict, references, openai_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_results(results):\n",
    "    \"\"\"Analyze evaluation results\"\"\"\n",
    "    evaluations = results.evaluations\n",
    "    \n",
    "    # Basic statistics\n",
    "    total = len(evaluations)\n",
    "    adequately_supported = sum(1 for e in evaluations if e.is_adequately_supported)\n",
    "    inadequately_supported = total - adequately_supported\n",
    "    \n",
    "    print(f\"Total claims evaluated: {total}\")\n",
    "    print(f\"Adequately supported: {adequately_supported} ({adequately_supported/total:.1%})\")\n",
    "    print(f\"Inadequately supported: {inadequately_supported} ({inadequately_supported/total:.1%})\")\n",
    "    \n",
    "    # Single vs multiple citation comparison\n",
    "    single_citation_claims = [e for e in evaluations if len(e.citation_keys) == 1]\n",
    "    multi_citation_claims = [e for e in evaluations if len(e.citation_keys) > 1]\n",
    "    \n",
    "    # Calculate adequacy rates\n",
    "    single_citation_adequacy = sum(1 for e in single_citation_claims if e.is_adequately_supported) / len(single_citation_claims) if single_citation_claims else 0\n",
    "    multi_citation_adequacy = sum(1 for e in multi_citation_claims if e.is_adequately_supported) / len(multi_citation_claims) if multi_citation_claims else 0\n",
    "    \n",
    "    print(f\"\\nSingle citation claims: {len(single_citation_claims)}\")\n",
    "    print(f\"Adequately supported: {single_citation_adequacy:.1%}\")\n",
    "    \n",
    "    print(f\"\\nMultiple citation claims: {len(multi_citation_claims)}\")\n",
    "    print(f\"Adequately supported: {multi_citation_adequacy:.1%}\")\n",
    "    \n",
    "    # Suggestions count\n",
    "    claims_with_suggestions = sum(1 for e in evaluations if e.suggestions and len(e.suggestions) > 0)\n",
    "    total_suggestions = sum(len(e.suggestions) for e in evaluations if e.suggestions)\n",
    "    \n",
    "    print(f\"\\nClaims with improvement suggestions: {claims_with_suggestions}\")\n",
    "    print(f\"Total improvement suggestions: {total_suggestions}\")\n",
    "\n",
    "# Load and analyze results if file exists\n",
    "evaluation_path = \"citation_analysis/evaluation.json\"\n",
    "if Path(evaluation_path).exists():\n",
    "    with open(evaluation_path, 'r', encoding='utf-8') as f:\n",
    "        saved_results = json.load(f)\n",
    "    \n",
    "    # Convert to EvaluationResults object\n",
    "    results = EvaluationResults.model_validate(saved_results)\n",
    "    analyze_results(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}